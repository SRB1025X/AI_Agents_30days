
# ğŸ¤ Conversational Voice Agent

> ğŸ—£ï¸ Talk to your AI â€” get human-like responses back in real time.Â Â 
> Powered by **AssemblyAI** (STT) + **Gemini (REST)** (LLM) + **Murf** (TTS).Â Â 
> Built as part of **#30DaysofVoiceAgents** by Murf AI.

<p align="center">
Â  <img src="static/Home.png" alt="UI Screenshot" width="720" />
  <img src="static/Result.png" alt="UI Screenshot" width="720" />
</p>

---

## âœ¨ Whatâ€™s inside

- ğŸ™ï¸ **Hands-free voice chat** â€” record in browser, hear responses automatically
- ğŸ§  **Multi-turn memory** â€” session-based chat history with `session_id`
- ğŸ›¡ï¸ **Resilient by design** â€” graceful fallbacks for STT/LLM/TTS failures
- âš¡ **FastAPI backend** â€” simple endpoints for TTS, STT, LLM, and agent chat
- ğŸ¨ **Tailwind UI** â€” clean, responsive, and mobile-friendly
- ğŸ” **Auto-continue** â€” re-starts recording after AI audio finishes

---

## ğŸ§© Architecture (high level)

<p align="center">
Â  <img src="static/Architecture.png" alt="UI Screenshot" width="720" />
</p>

```text
User Voice ğŸ¤
      |
      v
[Browser UI]  --(audio/webm)-->  [FastAPI Server]
                                       |
                                       +--> AssemblyAI (STT)  -- transcript -->
                                       |
                                 chat history (session_id)
                                       |
                                       +--> Gemini (REST)  -- LLM text ------+
                                                                            |
                                                                            v
                                       Murf (TTS)  -- mp3 URL ---> [Browser ğŸ”Š]
````

### Mermaid (sequence view)

```mermaid
sequenceDiagram
  participant U as User
  participant B as Browser UI
  participant S as FastAPI Server
  participant A as AssemblyAI (STT)
  participant G as Gemini (LLM)
  participant M as Murf (TTS)

  U->>B: Speak
  B->>S: POST /agent/chat/{session_id} (audio/webm)
  S->>A: Transcribe
  A-->>S: Transcript
  S->>G: Generate reply (history-aware)
  G-->>S: LLM response text
  S->>M: TTS generate (mp3)
  M-->>S: audioFile URL
  S-->>B: { transcript, llm_text, audio_url }
  B->>B: Play audio + show text
  B->>U: Auto-start next recording
```

### ğŸ—‚ Project Structure

```bash
.
â”œâ”€ main.py                # FastAPI app (STT/LLM/TTS endpoints + agent chat)
â”œâ”€ templates/
â”‚  â””â”€ index.html           # Frontend (Tailwind)
â”œâ”€ static/
â”‚  â”œâ”€ script.js            # Frontend logic (recording, fetch, playback)
â”‚  â”œâ”€ fallback.mp3         # Fallback audio when APIs fail
â”‚  â””â”€ screenshots/
â”‚     â”œâ”€ ui.png             # UI screenshot (add your own)
â”‚     â””â”€ architecture.png   # Architecture diagram (optional)
â”œâ”€ .env                   # API keys (NOT committed)
â”œâ”€ requirements.txt
â””â”€ README.md
```

### ğŸ”‘ Environment Variables

Create a `.env` in project root:

```env
MURF_API_KEY=your_murf_api_key_here
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
```

**Tip**: Never commit your `.env`. Use `.gitignore`.

-----

## âš™ï¸ Setup & Run

1.  **Clone & venv**

    ```bash
    git clone [https://github.com/SRB1025X/AI_Agents_30days](https://github.com/SRB1025X/AI_Agents_30days)
    cd AI_Agents_30days

    python -m venv venv
    # macOS/Linux
    source venv/bin/activate
    # Windows
    venv\Scripts\activate
    ```

2.  **Install deps**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Start the server**

    ```bash
    uvicorn main:app --reload
    ```

    Open: `http://127.0.0.1:8000`

-----

## ğŸ”Œ API Endpoints (Quick Reference)

**`POST /generate-audio`**

  - **Body**: `{"text": "Hello world"}`
  - **Resp**: `{"ok": true, "audio_url": "https://..."}` (falls back to `/static/fallback.mp3` if needed)

**`POST /transcribe/file`**

  - **FormData**: `file` (audio/webm)
  - **Resp**: `{"ok": true, "transcript": "..."}`

**`POST /llm/query`** (optional, single-turn)

  - **FormData**: `file` (audio/webm)
  - **Resp**: `{"ok": true, "llm_text": "...", "audio_url": "https://...", "transcript": "..."}`

**`POST /agent/chat/{session_id}`**

  - **FormData**: `file` (audio/webm)
  - **Resp**: `{"ok": true, "transcript": "...", "llm_text": "...", "audio_url": "https://..."}`
  - Stores conversation in memory (per `session_id`), sends full history to Gemini (REST), synthesizes Murf mp3.

-----

## ğŸ§ª CURL Smoke Tests

Replace `YOUR_SESSION_ID` first.

```bash
# Health check (HTML)
curl -s [http://127.0.0.1:8000](http://127.0.0.1:8000) | head -n 5

# TTS test
curl -s -X POST [http://127.0.0.1:8000/generate-audio](http://127.0.0.1:8000/generate-audio) \
  -H 'Content-Type: application/json' \
  -d '{"text":"Hello from the voice agent!"}'
```

-----

## ğŸ§± Error Handling & Fallbacks

Every external call (STT / LLM / TTS) is wrapped in `try/except`.

If something fails, the server returns a structured error with `stage` + `error`.

The client displays a message and auto-plays `static/fallback.mp3` so the experience never â€œgoes silentâ€.

You can simulate outages by temporarily removing an API key from `.env` and restarting.

**Example server error JSON**

```json
{
  "ok": false,
  "stage": "llm",
  "error": "RuntimeError: Gemini HTTP 403: ...details..."
}
```

-----

## ğŸ§­ Browser Notes (Audio)

  - The app records using `MediaRecorder`: `audio/webm;codecs=opus`.
  - Most Chromium browsers support this. Safari users may need to enable microphone permissions and test codec support.
  - Autoplay policies vary â€” the app attempts to play after user gesture (record/stop).

-----

## ğŸ§° Requirements (suggested)

```text
fastapi
uvicorn
python-dotenv
requests
assemblyai
jinja2
```

(Your `requirements.txt` may include more, depending on your setup.)

-----

## ğŸ§­ Roadmap

  - [ ] Replace in-memory history with a durable store (SQLite/Redis/Firestore)
  - [ ] Multi-voice & style controls for Murf
  - [ ] Streamed STT + streamed TTS
  - [ ] Live waveform + VU meter
  - [ ] Simple admin dashboard for session logs


-----

## ğŸ™ Credits

  - **Murf AI** â€” Text-to-Speech
  - **AssemblyAI** â€” Speech-to-Text
  - **Gemini (REST)** â€” LLM Responses
  - **FastAPI** â€” Web Framework

-----

## ğŸ“œ License

MIT â€” see `LICENSE`.
